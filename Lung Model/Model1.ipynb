{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3yLEFnXlhYwD"
      },
      "source": [
        "Disease Classification by CNN using MFCC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "L4a1oEyqTk52"
      },
      "outputs": [],
      "source": [
        "# Load various imports \n",
        "from datetime import datetime\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "\n",
        "import librosa\n",
        "import librosa.display\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Conv2D, MaxPooling2D, GlobalAveragePooling2D\n",
        "from keras.utils import to_categorical\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "\n",
        "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H-kDpsF-he6y",
        "outputId": "3ad8d436-4f45-45f1-f70e-fa4c6ae71f85"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "jlC2kwVBrh_T"
      },
      "outputs": [],
      "source": [
        "mypath = \"/content/gdrive/MyDrive/IEEE/IoT Stethescope/archive (1)/Respiratory_Sound_Database/Respiratory_Sound_Database/audio_and_txt_files/\"\n",
        "filenames = [f for f in listdir(mypath) if (isfile(join(mypath, f)) and f.endswith('.wav'))] "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "K6sISjrer3o2"
      },
      "outputs": [],
      "source": [
        "p_id_in_file = [] # patient IDs corresponding to each file\n",
        "for name in filenames:\n",
        "    p_id_in_file.append(int(name[:3]))\n",
        "\n",
        "p_id_in_file = np.array(p_id_in_file) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "pRciOs_tr6wc"
      },
      "outputs": [],
      "source": [
        "max_pad_len = 862 # to make the length of all MFCC equal\n",
        "\n",
        "def extract_features(file_name):\n",
        "    \"\"\"\n",
        "    This function takes in the path for an audio file as a string, loads it, and returns the MFCC\n",
        "    of the audio\"\"\"\n",
        "   \n",
        "    try:\n",
        "        audio, sample_rate = librosa.load(file_name, res_type='kaiser_fast', duration=20) \n",
        "        mfccs = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=40)\n",
        "        pad_width = max_pad_len - mfccs.shape[1]\n",
        "        mfccs = np.pad(mfccs, pad_width=((0, 0), (0, pad_width)), mode='constant')\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(\"Error encountered while parsing file: \", file_name)\n",
        "        return None \n",
        "     \n",
        "    return mfccs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "okpG-OOhsdLD"
      },
      "outputs": [],
      "source": [
        "filepaths = [join(mypath, f) for f in filenames] # full paths of files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "qMe52qmYsfuj"
      },
      "outputs": [],
      "source": [
        "p_diag = pd.read_csv(\"/content/gdrive/MyDrive/IEEE/IoT Stethescope/archive (1)/Respiratory_Sound_Database/Respiratory_Sound_Database/patient_diagnosis.csv\",header=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "GWYbWKSauNRb"
      },
      "outputs": [],
      "source": [
        "labels = np.array([p_diag[p_diag[0] == x][1].values[0] for x in p_id_in_file]) # labels for audio files"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "features = [] \n",
        "\n",
        "# Iterate through each sound file and extract the features\n",
        "for file_name in filepaths:\n",
        "    data = extract_features(file_name)\n",
        "    features.append(data)\n",
        "\n",
        "print('Finished feature extraction from ', len(features), ' files')\n",
        "features = np.array(features)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EXVf6KwOvdwO",
        "outputId": "b1338a33-aae9-4a41-de42-a2b2f68a4589"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Finished feature extraction from  920  files\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# plot an MFCC\n",
        "plt.figure(figsize=(10, 4))\n",
        "librosa.display.specshow(features[7], x_axis='time')\n",
        "plt.colorbar()\n",
        "plt.title('MFCC')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "zeTpjZd1vf0M",
        "outputId": "553b5d20-db22-4282-8125-b4c060a83384"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "features = np.array(features) # convert to numpy array"
      ],
      "metadata": {
        "id": "6VtD0cmVvlEh"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# delete the very rare diseases\n",
        "features1 = np.delete(features, np.where((labels == 'Asthma') | (labels == 'LRTI'))[0], axis=0) \n",
        "\n",
        "labels1 = np.delete(labels, np.where((labels == 'Asthma') | (labels == 'LRTI'))[0], axis=0)"
      ],
      "metadata": {
        "id": "5XFIupY7voyd"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print class counts\n",
        "unique_elements, counts_elements = np.unique(labels1, return_counts=True)\n",
        "print(np.asarray((unique_elements, counts_elements)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CYHRZ01Svpa9",
        "outputId": "bac96fee-f266-4880-aa13-b515e28a5718"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['Bronchiectasis' 'Bronchiolitis' 'COPD' 'Healthy' 'Pneumonia' 'URTI']\n",
            " ['16' '13' '793' '35' '37' '23']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# plot class counts\n",
        "y_pos = np.arange(len(unique_elements))\n",
        "plt.figure(figsize=(12,8))\n",
        "plt.bar(unique_elements, counts_elements, align='center', alpha=0.5)\n",
        "plt.xticks(y_pos, unique_elements)\n",
        "plt.ylabel('Count')\n",
        "plt.xlabel('Disease')\n",
        "plt.title('Disease Count in Sound Files (No Asthma or LRTI)')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 513
        },
        "id": "xCIawLtfvtAD",
        "outputId": "12cadfa7-9f71-4f69-d17e-52a6dd19bb63"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 864x576 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# One-hot encode labels\n",
        "le = LabelEncoder()\n",
        "i_labels = le.fit_transform(labels1)\n",
        "oh_labels = to_categorical(i_labels) "
      ],
      "metadata": {
        "id": "kr7SnykNx-LE"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# add channel dimension for CNN\n",
        "features1 = np.reshape(features1, (*features1.shape,1)) "
      ],
      "metadata": {
        "id": "3w8zOq_UyANd"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train test split\n",
        "x_train, x_test, y_train, y_test = train_test_split(features1, oh_labels, stratify=oh_labels, \n",
        "                                                    test_size=0.2, random_state = 42)"
      ],
      "metadata": {
        "id": "WE3VbZAtyB8b"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_rows = 40\n",
        "num_columns = 862\n",
        "num_channels = 1\n",
        "\n",
        "num_labels = oh_labels.shape[1]\n",
        "filter_size = 2\n",
        "\n",
        "# Construct model \n",
        "model = Sequential()\n",
        "model.add(Conv2D(filters=16, kernel_size=filter_size,\n",
        "                 input_shape=(num_rows, num_columns, num_channels), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=2))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Conv2D(filters=32, kernel_size=filter_size, activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=2))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Conv2D(filters=64, kernel_size=filter_size, activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=2))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Conv2D(filters=128, kernel_size=filter_size, activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=2))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(GlobalAveragePooling2D())\n",
        "\n",
        "model.add(Dense(num_labels, activation='softmax')) "
      ],
      "metadata": {
        "id": "zzDAiH8kyDZU"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile the model\n",
        "model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam')"
      ],
      "metadata": {
        "id": "Kh4BxLxnyGO9"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display model architecture summary \n",
        "model.summary()\n",
        "\n",
        "# Calculate pre-training accuracy \n",
        "score = model.evaluate(x_test, y_test, verbose=1)\n",
        "accuracy = 100*score[1]\n",
        "\n",
        "print(\"Pre-training accuracy: %.4f%%\" % accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5iQUG9mcyH07",
        "outputId": "e03d7b5e-9d19-4ac1-f210-8af8a28278b8"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 39, 861, 16)       80        \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2D  (None, 19, 430, 16)      0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 19, 430, 16)       0         \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 18, 429, 32)       2080      \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPooling  (None, 9, 214, 32)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 9, 214, 32)        0         \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 8, 213, 64)        8256      \n",
            "                                                                 \n",
            " max_pooling2d_2 (MaxPooling  (None, 4, 106, 64)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 4, 106, 64)        0         \n",
            "                                                                 \n",
            " conv2d_3 (Conv2D)           (None, 3, 105, 128)       32896     \n",
            "                                                                 \n",
            " max_pooling2d_3 (MaxPooling  (None, 1, 52, 128)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " dropout_3 (Dropout)         (None, 1, 52, 128)        0         \n",
            "                                                                 \n",
            " global_average_pooling2d (G  (None, 128)              0         \n",
            " lobalAveragePooling2D)                                          \n",
            "                                                                 \n",
            " dense (Dense)               (None, 6)                 774       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 44,086\n",
            "Trainable params: 44,086\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "6/6 [==============================] - 8s 44ms/step - loss: 24.2859 - accuracy: 0.0163\n",
            "Pre-training accuracy: 1.6304%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# train model\n",
        "num_epochs = 250\n",
        "num_batch_size = 128\n",
        "\n",
        "callbacks = [\n",
        "    ModelCheckpoint(\n",
        "        filepath='mymodel2_{epoch:02d}.h5',\n",
        "        # Path where to save the model\n",
        "        # The two parameters below mean that we will overwrite\n",
        "        # the current checkpoint if and only if\n",
        "        # the `val_accuracy` score has improved.\n",
        "        save_best_only=True,\n",
        "        monitor='val_accuracy',\n",
        "        verbose=1)\n",
        "]\n",
        "start = datetime.now()\n",
        "\n",
        "model.fit(x_train, y_train, batch_size=num_batch_size, epochs=num_epochs,\n",
        "          validation_data=(x_test, y_test), callbacks=callbacks, verbose=1)\n",
        "\n",
        "\n",
        "duration = datetime.now() - start\n",
        "print(\"Training completed in time: \", duration)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j48bEJWJyLdj",
        "outputId": "e7cff4fb-1cd1-42c6-f490-de5ea2c410e2"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.6047 - accuracy: 0.8622\n",
            "Epoch 1: val_accuracy improved from -inf to 0.26630, saving model to mymodel2_01.h5\n",
            "6/6 [==============================] - 1s 100ms/step - loss: 0.6047 - accuracy: 0.8622 - val_loss: 1.6559 - val_accuracy: 0.2663\n",
            "Epoch 2/250\n",
            "5/6 [========================>.....] - ETA: 0s - loss: 0.5705 - accuracy: 0.8547\n",
            "Epoch 2: val_accuracy improved from 0.26630 to 0.44565, saving model to mymodel2_02.h5\n",
            "6/6 [==============================] - 0s 83ms/step - loss: 0.5707 - accuracy: 0.8554 - val_loss: 1.4675 - val_accuracy: 0.4457\n",
            "Epoch 3/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.5656 - accuracy: 0.8581\n",
            "Epoch 3: val_accuracy improved from 0.44565 to 0.59239, saving model to mymodel2_03.h5\n",
            "6/6 [==============================] - 0s 84ms/step - loss: 0.5656 - accuracy: 0.8581 - val_loss: 1.3170 - val_accuracy: 0.5924\n",
            "Epoch 4/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.5677 - accuracy: 0.8486\n",
            "Epoch 4: val_accuracy improved from 0.59239 to 0.60870, saving model to mymodel2_04.h5\n",
            "6/6 [==============================] - 0s 85ms/step - loss: 0.5677 - accuracy: 0.8486 - val_loss: 1.3505 - val_accuracy: 0.6087\n",
            "Epoch 5/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.5306 - accuracy: 0.8540\n",
            "Epoch 5: val_accuracy improved from 0.60870 to 0.65761, saving model to mymodel2_05.h5\n",
            "6/6 [==============================] - 1s 100ms/step - loss: 0.5306 - accuracy: 0.8540 - val_loss: 1.2492 - val_accuracy: 0.6576\n",
            "Epoch 6/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.5563 - accuracy: 0.8417\n",
            "Epoch 6: val_accuracy did not improve from 0.65761\n",
            "6/6 [==============================] - 1s 119ms/step - loss: 0.5563 - accuracy: 0.8417 - val_loss: 1.3038 - val_accuracy: 0.6250\n",
            "Epoch 7/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.5141 - accuracy: 0.8568\n",
            "Epoch 7: val_accuracy did not improve from 0.65761\n",
            "6/6 [==============================] - 1s 90ms/step - loss: 0.5141 - accuracy: 0.8568 - val_loss: 1.1934 - val_accuracy: 0.6522\n",
            "Epoch 8/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.5107 - accuracy: 0.8554\n",
            "Epoch 8: val_accuracy improved from 0.65761 to 0.66304, saving model to mymodel2_08.h5\n",
            "6/6 [==============================] - 1s 105ms/step - loss: 0.5107 - accuracy: 0.8554 - val_loss: 1.2117 - val_accuracy: 0.6630\n",
            "Epoch 9/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.5029 - accuracy: 0.8472\n",
            "Epoch 9: val_accuracy improved from 0.66304 to 0.72826, saving model to mymodel2_09.h5\n",
            "6/6 [==============================] - 1s 99ms/step - loss: 0.5029 - accuracy: 0.8472 - val_loss: 1.0987 - val_accuracy: 0.7283\n",
            "Epoch 10/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.4959 - accuracy: 0.8527\n",
            "Epoch 10: val_accuracy improved from 0.72826 to 0.75543, saving model to mymodel2_10.h5\n",
            "6/6 [==============================] - 1s 108ms/step - loss: 0.4959 - accuracy: 0.8527 - val_loss: 1.0286 - val_accuracy: 0.7554\n",
            "Epoch 11/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.5116 - accuracy: 0.8595\n",
            "Epoch 11: val_accuracy did not improve from 0.75543\n",
            "6/6 [==============================] - 1s 101ms/step - loss: 0.5116 - accuracy: 0.8595 - val_loss: 1.0846 - val_accuracy: 0.7228\n",
            "Epoch 12/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.4859 - accuracy: 0.8445\n",
            "Epoch 12: val_accuracy improved from 0.75543 to 0.78261, saving model to mymodel2_12.h5\n",
            "6/6 [==============================] - 1s 111ms/step - loss: 0.4859 - accuracy: 0.8445 - val_loss: 0.8896 - val_accuracy: 0.7826\n",
            "Epoch 13/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.4619 - accuracy: 0.8513\n",
            "Epoch 13: val_accuracy did not improve from 0.78261\n",
            "6/6 [==============================] - 1s 90ms/step - loss: 0.4619 - accuracy: 0.8513 - val_loss: 0.9031 - val_accuracy: 0.7772\n",
            "Epoch 14/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.4642 - accuracy: 0.8554\n",
            "Epoch 14: val_accuracy improved from 0.78261 to 0.82065, saving model to mymodel2_14.h5\n",
            "6/6 [==============================] - 1s 108ms/step - loss: 0.4642 - accuracy: 0.8554 - val_loss: 0.7883 - val_accuracy: 0.8207\n",
            "Epoch 15/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.4485 - accuracy: 0.8636\n",
            "Epoch 15: val_accuracy did not improve from 0.82065\n",
            "6/6 [==============================] - 1s 87ms/step - loss: 0.4485 - accuracy: 0.8636 - val_loss: 0.8816 - val_accuracy: 0.7717\n",
            "Epoch 16/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.4414 - accuracy: 0.8513\n",
            "Epoch 16: val_accuracy improved from 0.82065 to 0.82609, saving model to mymodel2_16.h5\n",
            "6/6 [==============================] - 1s 112ms/step - loss: 0.4414 - accuracy: 0.8513 - val_loss: 0.7061 - val_accuracy: 0.8261\n",
            "Epoch 17/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.4437 - accuracy: 0.8554\n",
            "Epoch 17: val_accuracy did not improve from 0.82609\n",
            "6/6 [==============================] - 1s 98ms/step - loss: 0.4437 - accuracy: 0.8554 - val_loss: 0.7204 - val_accuracy: 0.8261\n",
            "Epoch 18/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.4230 - accuracy: 0.8581\n",
            "Epoch 18: val_accuracy did not improve from 0.82609\n",
            "6/6 [==============================] - 1s 95ms/step - loss: 0.4230 - accuracy: 0.8581 - val_loss: 0.6948 - val_accuracy: 0.8207\n",
            "Epoch 19/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.4040 - accuracy: 0.8636\n",
            "Epoch 19: val_accuracy did not improve from 0.82609\n",
            "6/6 [==============================] - 1s 91ms/step - loss: 0.4040 - accuracy: 0.8636 - val_loss: 0.6447 - val_accuracy: 0.8261\n",
            "Epoch 20/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.3948 - accuracy: 0.8677\n",
            "Epoch 20: val_accuracy did not improve from 0.82609\n",
            "6/6 [==============================] - 1s 93ms/step - loss: 0.3948 - accuracy: 0.8677 - val_loss: 0.6457 - val_accuracy: 0.8207\n",
            "Epoch 21/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.3934 - accuracy: 0.8690\n",
            "Epoch 21: val_accuracy did not improve from 0.82609\n",
            "6/6 [==============================] - 1s 90ms/step - loss: 0.3934 - accuracy: 0.8690 - val_loss: 0.6240 - val_accuracy: 0.8261\n",
            "Epoch 22/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.3796 - accuracy: 0.8731\n",
            "Epoch 22: val_accuracy did not improve from 0.82609\n",
            "6/6 [==============================] - 1s 85ms/step - loss: 0.3796 - accuracy: 0.8731 - val_loss: 0.6266 - val_accuracy: 0.8207\n",
            "Epoch 23/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.3824 - accuracy: 0.8649\n",
            "Epoch 23: val_accuracy improved from 0.82609 to 0.84239, saving model to mymodel2_23.h5\n",
            "6/6 [==============================] - 0s 84ms/step - loss: 0.3824 - accuracy: 0.8649 - val_loss: 0.5831 - val_accuracy: 0.8424\n",
            "Epoch 24/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.3763 - accuracy: 0.8704\n",
            "Epoch 24: val_accuracy improved from 0.84239 to 0.85326, saving model to mymodel2_24.h5\n",
            "6/6 [==============================] - 0s 86ms/step - loss: 0.3763 - accuracy: 0.8704 - val_loss: 0.5698 - val_accuracy: 0.8533\n",
            "Epoch 25/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.3812 - accuracy: 0.8690\n",
            "Epoch 25: val_accuracy did not improve from 0.85326\n",
            "6/6 [==============================] - 0s 79ms/step - loss: 0.3812 - accuracy: 0.8690 - val_loss: 0.6370 - val_accuracy: 0.8152\n",
            "Epoch 26/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.3734 - accuracy: 0.8704\n",
            "Epoch 26: val_accuracy improved from 0.85326 to 0.86413, saving model to mymodel2_26.h5\n",
            "6/6 [==============================] - 0s 85ms/step - loss: 0.3734 - accuracy: 0.8704 - val_loss: 0.5314 - val_accuracy: 0.8641\n",
            "Epoch 27/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.3662 - accuracy: 0.8827\n",
            "Epoch 27: val_accuracy did not improve from 0.86413\n",
            "6/6 [==============================] - 0s 82ms/step - loss: 0.3662 - accuracy: 0.8827 - val_loss: 0.5857 - val_accuracy: 0.8315\n",
            "Epoch 28/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.3695 - accuracy: 0.8636\n",
            "Epoch 28: val_accuracy did not improve from 0.86413\n",
            "6/6 [==============================] - 0s 78ms/step - loss: 0.3695 - accuracy: 0.8636 - val_loss: 0.5298 - val_accuracy: 0.8533\n",
            "Epoch 29/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.3523 - accuracy: 0.8745\n",
            "Epoch 29: val_accuracy did not improve from 0.86413\n",
            "6/6 [==============================] - 0s 78ms/step - loss: 0.3523 - accuracy: 0.8745 - val_loss: 0.5785 - val_accuracy: 0.8424\n",
            "Epoch 30/250\n",
            "5/6 [========================>.....] - ETA: 0s - loss: 0.3823 - accuracy: 0.8578\n",
            "Epoch 30: val_accuracy did not improve from 0.86413\n",
            "6/6 [==============================] - 0s 79ms/step - loss: 0.3568 - accuracy: 0.8663 - val_loss: 0.5098 - val_accuracy: 0.8641\n",
            "Epoch 31/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.3414 - accuracy: 0.8827\n",
            "Epoch 31: val_accuracy did not improve from 0.86413\n",
            "6/6 [==============================] - 0s 78ms/step - loss: 0.3414 - accuracy: 0.8827 - val_loss: 0.5567 - val_accuracy: 0.8533\n",
            "Epoch 32/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.3402 - accuracy: 0.8704\n",
            "Epoch 32: val_accuracy did not improve from 0.86413\n",
            "6/6 [==============================] - 0s 79ms/step - loss: 0.3402 - accuracy: 0.8704 - val_loss: 0.5053 - val_accuracy: 0.8641\n",
            "Epoch 33/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.3289 - accuracy: 0.8786\n",
            "Epoch 33: val_accuracy did not improve from 0.86413\n",
            "6/6 [==============================] - 0s 79ms/step - loss: 0.3289 - accuracy: 0.8786 - val_loss: 0.5097 - val_accuracy: 0.8641\n",
            "Epoch 34/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.3212 - accuracy: 0.8840\n",
            "Epoch 34: val_accuracy did not improve from 0.86413\n",
            "6/6 [==============================] - 0s 78ms/step - loss: 0.3212 - accuracy: 0.8840 - val_loss: 0.5038 - val_accuracy: 0.8587\n",
            "Epoch 35/250\n",
            "5/6 [========================>.....] - ETA: 0s - loss: 0.3317 - accuracy: 0.8797\n",
            "Epoch 35: val_accuracy did not improve from 0.86413\n",
            "6/6 [==============================] - 0s 77ms/step - loss: 0.3282 - accuracy: 0.8854 - val_loss: 0.5387 - val_accuracy: 0.8370\n",
            "Epoch 36/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.3250 - accuracy: 0.8827\n",
            "Epoch 36: val_accuracy did not improve from 0.86413\n",
            "6/6 [==============================] - 0s 77ms/step - loss: 0.3250 - accuracy: 0.8827 - val_loss: 0.4801 - val_accuracy: 0.8478\n",
            "Epoch 37/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.3210 - accuracy: 0.8813\n",
            "Epoch 37: val_accuracy improved from 0.86413 to 0.87500, saving model to mymodel2_37.h5\n",
            "6/6 [==============================] - 0s 85ms/step - loss: 0.3210 - accuracy: 0.8813 - val_loss: 0.4820 - val_accuracy: 0.8750\n",
            "Epoch 38/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.3062 - accuracy: 0.8895\n",
            "Epoch 38: val_accuracy did not improve from 0.87500\n",
            "6/6 [==============================] - 0s 81ms/step - loss: 0.3062 - accuracy: 0.8895 - val_loss: 0.4792 - val_accuracy: 0.8750\n",
            "Epoch 39/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.3084 - accuracy: 0.8827\n",
            "Epoch 39: val_accuracy did not improve from 0.87500\n",
            "6/6 [==============================] - 0s 79ms/step - loss: 0.3084 - accuracy: 0.8827 - val_loss: 0.4914 - val_accuracy: 0.8533\n",
            "Epoch 40/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.3175 - accuracy: 0.8854\n",
            "Epoch 40: val_accuracy did not improve from 0.87500\n",
            "6/6 [==============================] - 0s 83ms/step - loss: 0.3175 - accuracy: 0.8854 - val_loss: 0.4808 - val_accuracy: 0.8696\n",
            "Epoch 41/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.3087 - accuracy: 0.8868\n",
            "Epoch 41: val_accuracy did not improve from 0.87500\n",
            "6/6 [==============================] - 0s 82ms/step - loss: 0.3087 - accuracy: 0.8868 - val_loss: 0.4762 - val_accuracy: 0.8478\n",
            "Epoch 42/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.3193 - accuracy: 0.8799\n",
            "Epoch 42: val_accuracy did not improve from 0.87500\n",
            "6/6 [==============================] - 0s 79ms/step - loss: 0.3193 - accuracy: 0.8799 - val_loss: 0.4626 - val_accuracy: 0.8750\n",
            "Epoch 43/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.3121 - accuracy: 0.8813\n",
            "Epoch 43: val_accuracy did not improve from 0.87500\n",
            "6/6 [==============================] - 0s 79ms/step - loss: 0.3121 - accuracy: 0.8813 - val_loss: 0.4776 - val_accuracy: 0.8587\n",
            "Epoch 44/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.3043 - accuracy: 0.8895\n",
            "Epoch 44: val_accuracy did not improve from 0.87500\n",
            "6/6 [==============================] - 0s 83ms/step - loss: 0.3043 - accuracy: 0.8895 - val_loss: 0.4622 - val_accuracy: 0.8641\n",
            "Epoch 45/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.3105 - accuracy: 0.8854\n",
            "Epoch 45: val_accuracy did not improve from 0.87500\n",
            "6/6 [==============================] - 0s 80ms/step - loss: 0.3105 - accuracy: 0.8854 - val_loss: 0.4748 - val_accuracy: 0.8478\n",
            "Epoch 46/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.3003 - accuracy: 0.8813\n",
            "Epoch 46: val_accuracy did not improve from 0.87500\n",
            "6/6 [==============================] - 0s 78ms/step - loss: 0.3003 - accuracy: 0.8813 - val_loss: 0.4443 - val_accuracy: 0.8641\n",
            "Epoch 47/250\n",
            "5/6 [========================>.....] - ETA: 0s - loss: 0.2994 - accuracy: 0.8859\n",
            "Epoch 47: val_accuracy did not improve from 0.87500\n",
            "6/6 [==============================] - 0s 77ms/step - loss: 0.3063 - accuracy: 0.8868 - val_loss: 0.5000 - val_accuracy: 0.8478\n",
            "Epoch 48/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.3181 - accuracy: 0.8799\n",
            "Epoch 48: val_accuracy did not improve from 0.87500\n",
            "6/6 [==============================] - 0s 78ms/step - loss: 0.3181 - accuracy: 0.8799 - val_loss: 0.4464 - val_accuracy: 0.8533\n",
            "Epoch 49/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.3007 - accuracy: 0.8895\n",
            "Epoch 49: val_accuracy did not improve from 0.87500\n",
            "6/6 [==============================] - 0s 79ms/step - loss: 0.3007 - accuracy: 0.8895 - val_loss: 0.4366 - val_accuracy: 0.8641\n",
            "Epoch 50/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.3056 - accuracy: 0.8840\n",
            "Epoch 50: val_accuracy did not improve from 0.87500\n",
            "6/6 [==============================] - 0s 79ms/step - loss: 0.3056 - accuracy: 0.8840 - val_loss: 0.4674 - val_accuracy: 0.8533\n",
            "Epoch 51/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.2905 - accuracy: 0.8881\n",
            "Epoch 51: val_accuracy did not improve from 0.87500\n",
            "6/6 [==============================] - 0s 79ms/step - loss: 0.2905 - accuracy: 0.8881 - val_loss: 0.4485 - val_accuracy: 0.8641\n",
            "Epoch 52/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.2940 - accuracy: 0.8827\n",
            "Epoch 52: val_accuracy did not improve from 0.87500\n",
            "6/6 [==============================] - 0s 78ms/step - loss: 0.2940 - accuracy: 0.8827 - val_loss: 0.4346 - val_accuracy: 0.8641\n",
            "Epoch 53/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.2872 - accuracy: 0.8881\n",
            "Epoch 53: val_accuracy did not improve from 0.87500\n",
            "6/6 [==============================] - 0s 79ms/step - loss: 0.2872 - accuracy: 0.8881 - val_loss: 0.4437 - val_accuracy: 0.8641\n",
            "Epoch 54/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.2779 - accuracy: 0.8936\n",
            "Epoch 54: val_accuracy did not improve from 0.87500\n",
            "6/6 [==============================] - 0s 80ms/step - loss: 0.2779 - accuracy: 0.8936 - val_loss: 0.4342 - val_accuracy: 0.8696\n",
            "Epoch 55/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.2759 - accuracy: 0.8881\n",
            "Epoch 55: val_accuracy did not improve from 0.87500\n",
            "6/6 [==============================] - 0s 79ms/step - loss: 0.2759 - accuracy: 0.8881 - val_loss: 0.4405 - val_accuracy: 0.8533\n",
            "Epoch 56/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.2740 - accuracy: 0.8909\n",
            "Epoch 56: val_accuracy did not improve from 0.87500\n",
            "6/6 [==============================] - 0s 78ms/step - loss: 0.2740 - accuracy: 0.8909 - val_loss: 0.4503 - val_accuracy: 0.8533\n",
            "Epoch 57/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.2803 - accuracy: 0.8895\n",
            "Epoch 57: val_accuracy did not improve from 0.87500\n",
            "6/6 [==============================] - 0s 80ms/step - loss: 0.2803 - accuracy: 0.8895 - val_loss: 0.4501 - val_accuracy: 0.8587\n",
            "Epoch 58/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.2939 - accuracy: 0.8895\n",
            "Epoch 58: val_accuracy did not improve from 0.87500\n",
            "6/6 [==============================] - 0s 81ms/step - loss: 0.2939 - accuracy: 0.8895 - val_loss: 0.4301 - val_accuracy: 0.8696\n",
            "Epoch 59/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.2759 - accuracy: 0.8950\n",
            "Epoch 59: val_accuracy did not improve from 0.87500\n",
            "6/6 [==============================] - 0s 83ms/step - loss: 0.2759 - accuracy: 0.8950 - val_loss: 0.4375 - val_accuracy: 0.8696\n",
            "Epoch 60/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.2678 - accuracy: 0.8963\n",
            "Epoch 60: val_accuracy did not improve from 0.87500\n",
            "6/6 [==============================] - 0s 79ms/step - loss: 0.2678 - accuracy: 0.8963 - val_loss: 0.4279 - val_accuracy: 0.8587\n",
            "Epoch 61/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.2614 - accuracy: 0.8950\n",
            "Epoch 61: val_accuracy did not improve from 0.87500\n",
            "6/6 [==============================] - 0s 80ms/step - loss: 0.2614 - accuracy: 0.8950 - val_loss: 0.4228 - val_accuracy: 0.8750\n",
            "Epoch 62/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.2841 - accuracy: 0.8922\n",
            "Epoch 62: val_accuracy did not improve from 0.87500\n",
            "6/6 [==============================] - 0s 80ms/step - loss: 0.2841 - accuracy: 0.8922 - val_loss: 0.4496 - val_accuracy: 0.8750\n",
            "Epoch 63/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.2738 - accuracy: 0.8881\n",
            "Epoch 63: val_accuracy did not improve from 0.87500\n",
            "6/6 [==============================] - 0s 83ms/step - loss: 0.2738 - accuracy: 0.8881 - val_loss: 0.4205 - val_accuracy: 0.8587\n",
            "Epoch 64/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.2651 - accuracy: 0.9004\n",
            "Epoch 64: val_accuracy did not improve from 0.87500\n",
            "6/6 [==============================] - 0s 79ms/step - loss: 0.2651 - accuracy: 0.9004 - val_loss: 0.4192 - val_accuracy: 0.8750\n",
            "Epoch 65/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.2702 - accuracy: 0.9004\n",
            "Epoch 65: val_accuracy did not improve from 0.87500\n",
            "6/6 [==============================] - 0s 83ms/step - loss: 0.2702 - accuracy: 0.9004 - val_loss: 0.4217 - val_accuracy: 0.8750\n",
            "Epoch 66/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.2824 - accuracy: 0.8909\n",
            "Epoch 66: val_accuracy did not improve from 0.87500\n",
            "6/6 [==============================] - 0s 79ms/step - loss: 0.2824 - accuracy: 0.8909 - val_loss: 0.4413 - val_accuracy: 0.8587\n",
            "Epoch 67/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.2890 - accuracy: 0.8909\n",
            "Epoch 67: val_accuracy did not improve from 0.87500\n",
            "6/6 [==============================] - 0s 80ms/step - loss: 0.2890 - accuracy: 0.8909 - val_loss: 0.4363 - val_accuracy: 0.8533\n",
            "Epoch 68/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.2684 - accuracy: 0.8922\n",
            "Epoch 68: val_accuracy did not improve from 0.87500\n",
            "6/6 [==============================] - 0s 78ms/step - loss: 0.2684 - accuracy: 0.8922 - val_loss: 0.4069 - val_accuracy: 0.8750\n",
            "Epoch 69/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.2730 - accuracy: 0.8963\n",
            "Epoch 69: val_accuracy did not improve from 0.87500\n",
            "6/6 [==============================] - 0s 83ms/step - loss: 0.2730 - accuracy: 0.8963 - val_loss: 0.4362 - val_accuracy: 0.8587\n",
            "Epoch 70/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.2636 - accuracy: 0.9018\n",
            "Epoch 70: val_accuracy improved from 0.87500 to 0.88043, saving model to mymodel2_70.h5\n",
            "6/6 [==============================] - 0s 86ms/step - loss: 0.2636 - accuracy: 0.9018 - val_loss: 0.4047 - val_accuracy: 0.8804\n",
            "Epoch 71/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.2824 - accuracy: 0.8936\n",
            "Epoch 71: val_accuracy did not improve from 0.88043\n",
            "6/6 [==============================] - 0s 79ms/step - loss: 0.2824 - accuracy: 0.8936 - val_loss: 0.4200 - val_accuracy: 0.8696\n",
            "Epoch 72/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.2979 - accuracy: 0.8868\n",
            "Epoch 72: val_accuracy did not improve from 0.88043\n",
            "6/6 [==============================] - 0s 80ms/step - loss: 0.2979 - accuracy: 0.8868 - val_loss: 0.4758 - val_accuracy: 0.8424\n",
            "Epoch 73/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.2621 - accuracy: 0.8950\n",
            "Epoch 73: val_accuracy did not improve from 0.88043\n",
            "6/6 [==============================] - 0s 80ms/step - loss: 0.2621 - accuracy: 0.8950 - val_loss: 0.4028 - val_accuracy: 0.8696\n",
            "Epoch 74/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.2649 - accuracy: 0.8963\n",
            "Epoch 74: val_accuracy did not improve from 0.88043\n",
            "6/6 [==============================] - 0s 80ms/step - loss: 0.2649 - accuracy: 0.8963 - val_loss: 0.4334 - val_accuracy: 0.8641\n",
            "Epoch 75/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.2631 - accuracy: 0.8963\n",
            "Epoch 75: val_accuracy did not improve from 0.88043\n",
            "6/6 [==============================] - 0s 81ms/step - loss: 0.2631 - accuracy: 0.8963 - val_loss: 0.4179 - val_accuracy: 0.8696\n",
            "Epoch 76/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.2451 - accuracy: 0.9100\n",
            "Epoch 76: val_accuracy did not improve from 0.88043\n",
            "6/6 [==============================] - 0s 81ms/step - loss: 0.2451 - accuracy: 0.9100 - val_loss: 0.4076 - val_accuracy: 0.8750\n",
            "Epoch 77/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.2482 - accuracy: 0.9018\n",
            "Epoch 77: val_accuracy did not improve from 0.88043\n",
            "6/6 [==============================] - 0s 80ms/step - loss: 0.2482 - accuracy: 0.9018 - val_loss: 0.4092 - val_accuracy: 0.8750\n",
            "Epoch 78/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.2345 - accuracy: 0.9168\n",
            "Epoch 78: val_accuracy did not improve from 0.88043\n",
            "6/6 [==============================] - 0s 80ms/step - loss: 0.2345 - accuracy: 0.9168 - val_loss: 0.4021 - val_accuracy: 0.8804\n",
            "Epoch 79/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.2364 - accuracy: 0.9127\n",
            "Epoch 79: val_accuracy did not improve from 0.88043\n",
            "6/6 [==============================] - 0s 79ms/step - loss: 0.2364 - accuracy: 0.9127 - val_loss: 0.4084 - val_accuracy: 0.8641\n",
            "Epoch 80/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.2360 - accuracy: 0.9086\n",
            "Epoch 80: val_accuracy did not improve from 0.88043\n",
            "6/6 [==============================] - 0s 80ms/step - loss: 0.2360 - accuracy: 0.9086 - val_loss: 0.3847 - val_accuracy: 0.8804\n",
            "Epoch 81/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.2376 - accuracy: 0.9100\n",
            "Epoch 81: val_accuracy did not improve from 0.88043\n",
            "6/6 [==============================] - 0s 78ms/step - loss: 0.2376 - accuracy: 0.9100 - val_loss: 0.3964 - val_accuracy: 0.8804\n",
            "Epoch 82/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.2443 - accuracy: 0.9018\n",
            "Epoch 82: val_accuracy did not improve from 0.88043\n",
            "6/6 [==============================] - 0s 81ms/step - loss: 0.2443 - accuracy: 0.9018 - val_loss: 0.3896 - val_accuracy: 0.8804\n",
            "Epoch 83/250\n",
            "5/6 [========================>.....] - ETA: 0s - loss: 0.2453 - accuracy: 0.9031\n",
            "Epoch 83: val_accuracy did not improve from 0.88043\n",
            "6/6 [==============================] - 0s 78ms/step - loss: 0.2322 - accuracy: 0.9113 - val_loss: 0.3806 - val_accuracy: 0.8804\n",
            "Epoch 84/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.2326 - accuracy: 0.9100\n",
            "Epoch 84: val_accuracy did not improve from 0.88043\n",
            "6/6 [==============================] - 0s 80ms/step - loss: 0.2326 - accuracy: 0.9100 - val_loss: 0.4102 - val_accuracy: 0.8587\n",
            "Epoch 85/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.2448 - accuracy: 0.9059\n",
            "Epoch 85: val_accuracy improved from 0.88043 to 0.88587, saving model to mymodel2_85.h5\n",
            "6/6 [==============================] - 1s 86ms/step - loss: 0.2448 - accuracy: 0.9059 - val_loss: 0.3799 - val_accuracy: 0.8859\n",
            "Epoch 86/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.2460 - accuracy: 0.9018\n",
            "Epoch 86: val_accuracy did not improve from 0.88587\n",
            "6/6 [==============================] - 0s 80ms/step - loss: 0.2460 - accuracy: 0.9018 - val_loss: 0.3889 - val_accuracy: 0.8859\n",
            "Epoch 87/250\n",
            "5/6 [========================>.....] - ETA: 0s - loss: 0.2223 - accuracy: 0.9109\n",
            "Epoch 87: val_accuracy improved from 0.88587 to 0.89130, saving model to mymodel2_87.h5\n",
            "6/6 [==============================] - 0s 84ms/step - loss: 0.2348 - accuracy: 0.9045 - val_loss: 0.3733 - val_accuracy: 0.8913\n",
            "Epoch 88/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.2249 - accuracy: 0.9154\n",
            "Epoch 88: val_accuracy did not improve from 0.89130\n",
            "6/6 [==============================] - 0s 80ms/step - loss: 0.2249 - accuracy: 0.9154 - val_loss: 0.3815 - val_accuracy: 0.8804\n",
            "Epoch 89/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.2448 - accuracy: 0.8990\n",
            "Epoch 89: val_accuracy did not improve from 0.89130\n",
            "6/6 [==============================] - 0s 82ms/step - loss: 0.2448 - accuracy: 0.8990 - val_loss: 0.3803 - val_accuracy: 0.8804\n",
            "Epoch 90/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.2192 - accuracy: 0.9168\n",
            "Epoch 90: val_accuracy did not improve from 0.89130\n",
            "6/6 [==============================] - 0s 79ms/step - loss: 0.2192 - accuracy: 0.9168 - val_loss: 0.3718 - val_accuracy: 0.8804\n",
            "Epoch 91/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.2182 - accuracy: 0.9141\n",
            "Epoch 91: val_accuracy did not improve from 0.89130\n",
            "6/6 [==============================] - 0s 83ms/step - loss: 0.2182 - accuracy: 0.9141 - val_loss: 0.3655 - val_accuracy: 0.8804\n",
            "Epoch 92/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.2203 - accuracy: 0.9127\n",
            "Epoch 92: val_accuracy did not improve from 0.89130\n",
            "6/6 [==============================] - 0s 80ms/step - loss: 0.2203 - accuracy: 0.9127 - val_loss: 0.3821 - val_accuracy: 0.8696\n",
            "Epoch 93/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.2115 - accuracy: 0.9154\n",
            "Epoch 93: val_accuracy did not improve from 0.89130\n",
            "6/6 [==============================] - 0s 84ms/step - loss: 0.2115 - accuracy: 0.9154 - val_loss: 0.3793 - val_accuracy: 0.8804\n",
            "Epoch 94/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.2241 - accuracy: 0.9127\n",
            "Epoch 94: val_accuracy did not improve from 0.89130\n",
            "6/6 [==============================] - 0s 80ms/step - loss: 0.2241 - accuracy: 0.9127 - val_loss: 0.3684 - val_accuracy: 0.8804\n",
            "Epoch 95/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.2090 - accuracy: 0.9209\n",
            "Epoch 95: val_accuracy did not improve from 0.89130\n",
            "6/6 [==============================] - 0s 84ms/step - loss: 0.2090 - accuracy: 0.9209 - val_loss: 0.3689 - val_accuracy: 0.8804\n",
            "Epoch 96/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.2166 - accuracy: 0.9100\n",
            "Epoch 96: val_accuracy did not improve from 0.89130\n",
            "6/6 [==============================] - 0s 80ms/step - loss: 0.2166 - accuracy: 0.9100 - val_loss: 0.3739 - val_accuracy: 0.8804\n",
            "Epoch 97/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.2013 - accuracy: 0.9277\n",
            "Epoch 97: val_accuracy did not improve from 0.89130\n",
            "6/6 [==============================] - 0s 79ms/step - loss: 0.2013 - accuracy: 0.9277 - val_loss: 0.3560 - val_accuracy: 0.8859\n",
            "Epoch 98/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.2171 - accuracy: 0.9100\n",
            "Epoch 98: val_accuracy did not improve from 0.89130\n",
            "6/6 [==============================] - 0s 79ms/step - loss: 0.2171 - accuracy: 0.9100 - val_loss: 0.3467 - val_accuracy: 0.8859\n",
            "Epoch 99/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.2102 - accuracy: 0.9168\n",
            "Epoch 99: val_accuracy did not improve from 0.89130\n",
            "6/6 [==============================] - 0s 84ms/step - loss: 0.2102 - accuracy: 0.9168 - val_loss: 0.3654 - val_accuracy: 0.8913\n",
            "Epoch 100/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1995 - accuracy: 0.9236\n",
            "Epoch 100: val_accuracy did not improve from 0.89130\n",
            "6/6 [==============================] - 0s 80ms/step - loss: 0.1995 - accuracy: 0.9236 - val_loss: 0.3454 - val_accuracy: 0.8804\n",
            "Epoch 101/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.2095 - accuracy: 0.9181\n",
            "Epoch 101: val_accuracy did not improve from 0.89130\n",
            "6/6 [==============================] - 0s 81ms/step - loss: 0.2095 - accuracy: 0.9181 - val_loss: 0.3596 - val_accuracy: 0.8859\n",
            "Epoch 102/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.2041 - accuracy: 0.9195\n",
            "Epoch 102: val_accuracy improved from 0.89130 to 0.90217, saving model to mymodel2_102.h5\n",
            "6/6 [==============================] - 0s 86ms/step - loss: 0.2041 - accuracy: 0.9195 - val_loss: 0.3411 - val_accuracy: 0.9022\n",
            "Epoch 103/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1995 - accuracy: 0.9209\n",
            "Epoch 103: val_accuracy did not improve from 0.90217\n",
            "6/6 [==============================] - 0s 79ms/step - loss: 0.1995 - accuracy: 0.9209 - val_loss: 0.3523 - val_accuracy: 0.8804\n",
            "Epoch 104/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.2152 - accuracy: 0.9195\n",
            "Epoch 104: val_accuracy did not improve from 0.90217\n",
            "6/6 [==============================] - 0s 79ms/step - loss: 0.2152 - accuracy: 0.9195 - val_loss: 0.3574 - val_accuracy: 0.8804\n",
            "Epoch 105/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.2160 - accuracy: 0.9113\n",
            "Epoch 105: val_accuracy did not improve from 0.90217\n",
            "6/6 [==============================] - 0s 81ms/step - loss: 0.2160 - accuracy: 0.9113 - val_loss: 0.3371 - val_accuracy: 0.8913\n",
            "Epoch 106/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.2011 - accuracy: 0.9100\n",
            "Epoch 106: val_accuracy did not improve from 0.90217\n",
            "6/6 [==============================] - 0s 80ms/step - loss: 0.2011 - accuracy: 0.9100 - val_loss: 0.3518 - val_accuracy: 0.8859\n",
            "Epoch 107/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1999 - accuracy: 0.9181\n",
            "Epoch 107: val_accuracy did not improve from 0.90217\n",
            "6/6 [==============================] - 0s 81ms/step - loss: 0.1999 - accuracy: 0.9181 - val_loss: 0.3533 - val_accuracy: 0.8750\n",
            "Epoch 108/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.2009 - accuracy: 0.9195\n",
            "Epoch 108: val_accuracy did not improve from 0.90217\n",
            "6/6 [==============================] - 0s 79ms/step - loss: 0.2009 - accuracy: 0.9195 - val_loss: 0.3473 - val_accuracy: 0.8967\n",
            "Epoch 109/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.2172 - accuracy: 0.9045\n",
            "Epoch 109: val_accuracy did not improve from 0.90217\n",
            "6/6 [==============================] - 0s 79ms/step - loss: 0.2172 - accuracy: 0.9045 - val_loss: 0.3501 - val_accuracy: 0.8804\n",
            "Epoch 110/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.2182 - accuracy: 0.9209\n",
            "Epoch 110: val_accuracy did not improve from 0.90217\n",
            "6/6 [==============================] - 0s 79ms/step - loss: 0.2182 - accuracy: 0.9209 - val_loss: 0.3761 - val_accuracy: 0.8804\n",
            "Epoch 111/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.2148 - accuracy: 0.9100\n",
            "Epoch 111: val_accuracy improved from 0.90217 to 0.90761, saving model to mymodel2_111.h5\n",
            "6/6 [==============================] - 1s 87ms/step - loss: 0.2148 - accuracy: 0.9100 - val_loss: 0.3345 - val_accuracy: 0.9076\n",
            "Epoch 112/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1868 - accuracy: 0.9236\n",
            "Epoch 112: val_accuracy did not improve from 0.90761\n",
            "6/6 [==============================] - 0s 81ms/step - loss: 0.1868 - accuracy: 0.9236 - val_loss: 0.3375 - val_accuracy: 0.9022\n",
            "Epoch 113/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1808 - accuracy: 0.9236\n",
            "Epoch 113: val_accuracy did not improve from 0.90761\n",
            "6/6 [==============================] - 0s 84ms/step - loss: 0.1808 - accuracy: 0.9236 - val_loss: 0.3458 - val_accuracy: 0.9076\n",
            "Epoch 114/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1935 - accuracy: 0.9236\n",
            "Epoch 114: val_accuracy did not improve from 0.90761\n",
            "6/6 [==============================] - 0s 80ms/step - loss: 0.1935 - accuracy: 0.9236 - val_loss: 0.3326 - val_accuracy: 0.9076\n",
            "Epoch 115/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1821 - accuracy: 0.9277\n",
            "Epoch 115: val_accuracy did not improve from 0.90761\n",
            "6/6 [==============================] - 0s 83ms/step - loss: 0.1821 - accuracy: 0.9277 - val_loss: 0.3537 - val_accuracy: 0.8967\n",
            "Epoch 116/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1943 - accuracy: 0.9195\n",
            "Epoch 116: val_accuracy did not improve from 0.90761\n",
            "6/6 [==============================] - 0s 81ms/step - loss: 0.1943 - accuracy: 0.9195 - val_loss: 0.3370 - val_accuracy: 0.9076\n",
            "Epoch 117/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1786 - accuracy: 0.9318\n",
            "Epoch 117: val_accuracy did not improve from 0.90761\n",
            "6/6 [==============================] - 0s 79ms/step - loss: 0.1786 - accuracy: 0.9318 - val_loss: 0.3488 - val_accuracy: 0.8750\n",
            "Epoch 118/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1810 - accuracy: 0.9291\n",
            "Epoch 118: val_accuracy did not improve from 0.90761\n",
            "6/6 [==============================] - 0s 80ms/step - loss: 0.1810 - accuracy: 0.9291 - val_loss: 0.3273 - val_accuracy: 0.9022\n",
            "Epoch 119/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1713 - accuracy: 0.9345\n",
            "Epoch 119: val_accuracy did not improve from 0.90761\n",
            "6/6 [==============================] - 0s 79ms/step - loss: 0.1713 - accuracy: 0.9345 - val_loss: 0.3458 - val_accuracy: 0.9022\n",
            "Epoch 120/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1848 - accuracy: 0.9277\n",
            "Epoch 120: val_accuracy did not improve from 0.90761\n",
            "6/6 [==============================] - 0s 80ms/step - loss: 0.1848 - accuracy: 0.9277 - val_loss: 0.3429 - val_accuracy: 0.8913\n",
            "Epoch 121/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.2122 - accuracy: 0.9154\n",
            "Epoch 121: val_accuracy did not improve from 0.90761\n",
            "6/6 [==============================] - 0s 81ms/step - loss: 0.2122 - accuracy: 0.9154 - val_loss: 0.3856 - val_accuracy: 0.8750\n",
            "Epoch 122/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.2040 - accuracy: 0.9168\n",
            "Epoch 122: val_accuracy did not improve from 0.90761\n",
            "6/6 [==============================] - 0s 81ms/step - loss: 0.2040 - accuracy: 0.9168 - val_loss: 0.3308 - val_accuracy: 0.8913\n",
            "Epoch 123/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1985 - accuracy: 0.9277\n",
            "Epoch 123: val_accuracy did not improve from 0.90761\n",
            "6/6 [==============================] - 0s 80ms/step - loss: 0.1985 - accuracy: 0.9277 - val_loss: 0.3518 - val_accuracy: 0.8804\n",
            "Epoch 124/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1908 - accuracy: 0.9236\n",
            "Epoch 124: val_accuracy did not improve from 0.90761\n",
            "6/6 [==============================] - 0s 83ms/step - loss: 0.1908 - accuracy: 0.9236 - val_loss: 0.3430 - val_accuracy: 0.9022\n",
            "Epoch 125/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1823 - accuracy: 0.9236\n",
            "Epoch 125: val_accuracy improved from 0.90761 to 0.91304, saving model to mymodel2_125.h5\n",
            "6/6 [==============================] - 1s 88ms/step - loss: 0.1823 - accuracy: 0.9236 - val_loss: 0.3282 - val_accuracy: 0.9130\n",
            "Epoch 126/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1706 - accuracy: 0.9345\n",
            "Epoch 126: val_accuracy did not improve from 0.91304\n",
            "6/6 [==============================] - 0s 78ms/step - loss: 0.1706 - accuracy: 0.9345 - val_loss: 0.3197 - val_accuracy: 0.9130\n",
            "Epoch 127/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1671 - accuracy: 0.9304\n",
            "Epoch 127: val_accuracy did not improve from 0.91304\n",
            "6/6 [==============================] - 0s 81ms/step - loss: 0.1671 - accuracy: 0.9304 - val_loss: 0.3417 - val_accuracy: 0.9076\n",
            "Epoch 128/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1666 - accuracy: 0.9359\n",
            "Epoch 128: val_accuracy did not improve from 0.91304\n",
            "6/6 [==============================] - 0s 84ms/step - loss: 0.1666 - accuracy: 0.9359 - val_loss: 0.3208 - val_accuracy: 0.9130\n",
            "Epoch 129/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1801 - accuracy: 0.9263\n",
            "Epoch 129: val_accuracy did not improve from 0.91304\n",
            "6/6 [==============================] - 0s 82ms/step - loss: 0.1801 - accuracy: 0.9263 - val_loss: 0.3304 - val_accuracy: 0.9130\n",
            "Epoch 130/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1826 - accuracy: 0.9209\n",
            "Epoch 130: val_accuracy did not improve from 0.91304\n",
            "6/6 [==============================] - 0s 81ms/step - loss: 0.1826 - accuracy: 0.9209 - val_loss: 0.3594 - val_accuracy: 0.8804\n",
            "Epoch 131/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1708 - accuracy: 0.9359\n",
            "Epoch 131: val_accuracy did not improve from 0.91304\n",
            "6/6 [==============================] - 0s 81ms/step - loss: 0.1708 - accuracy: 0.9359 - val_loss: 0.3492 - val_accuracy: 0.9076\n",
            "Epoch 132/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1790 - accuracy: 0.9291\n",
            "Epoch 132: val_accuracy did not improve from 0.91304\n",
            "6/6 [==============================] - 0s 81ms/step - loss: 0.1790 - accuracy: 0.9291 - val_loss: 0.3282 - val_accuracy: 0.9022\n",
            "Epoch 133/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1520 - accuracy: 0.9400\n",
            "Epoch 133: val_accuracy did not improve from 0.91304\n",
            "6/6 [==============================] - 0s 81ms/step - loss: 0.1520 - accuracy: 0.9400 - val_loss: 0.3264 - val_accuracy: 0.9022\n",
            "Epoch 134/250\n",
            "5/6 [========================>.....] - ETA: 0s - loss: 0.1540 - accuracy: 0.9453\n",
            "Epoch 134: val_accuracy did not improve from 0.91304\n",
            "6/6 [==============================] - 0s 78ms/step - loss: 0.1673 - accuracy: 0.9386 - val_loss: 0.3782 - val_accuracy: 0.8804\n",
            "Epoch 135/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1665 - accuracy: 0.9291\n",
            "Epoch 135: val_accuracy did not improve from 0.91304\n",
            "6/6 [==============================] - 0s 79ms/step - loss: 0.1665 - accuracy: 0.9291 - val_loss: 0.3307 - val_accuracy: 0.9130\n",
            "Epoch 136/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1557 - accuracy: 0.9386\n",
            "Epoch 136: val_accuracy did not improve from 0.91304\n",
            "6/6 [==============================] - 1s 100ms/step - loss: 0.1557 - accuracy: 0.9386 - val_loss: 0.3178 - val_accuracy: 0.8967\n",
            "Epoch 137/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1764 - accuracy: 0.9304\n",
            "Epoch 137: val_accuracy did not improve from 0.91304\n",
            "6/6 [==============================] - 1s 98ms/step - loss: 0.1764 - accuracy: 0.9304 - val_loss: 0.3665 - val_accuracy: 0.8913\n",
            "Epoch 138/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1664 - accuracy: 0.9345\n",
            "Epoch 138: val_accuracy did not improve from 0.91304\n",
            "6/6 [==============================] - 0s 81ms/step - loss: 0.1664 - accuracy: 0.9345 - val_loss: 0.3298 - val_accuracy: 0.9022\n",
            "Epoch 139/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1500 - accuracy: 0.9400\n",
            "Epoch 139: val_accuracy did not improve from 0.91304\n",
            "6/6 [==============================] - 1s 110ms/step - loss: 0.1500 - accuracy: 0.9400 - val_loss: 0.3612 - val_accuracy: 0.8804\n",
            "Epoch 140/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1581 - accuracy: 0.9359\n",
            "Epoch 140: val_accuracy did not improve from 0.91304\n",
            "6/6 [==============================] - 1s 106ms/step - loss: 0.1581 - accuracy: 0.9359 - val_loss: 0.3198 - val_accuracy: 0.9076\n",
            "Epoch 141/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1519 - accuracy: 0.9413\n",
            "Epoch 141: val_accuracy did not improve from 0.91304\n",
            "6/6 [==============================] - 1s 112ms/step - loss: 0.1519 - accuracy: 0.9413 - val_loss: 0.3336 - val_accuracy: 0.9022\n",
            "Epoch 142/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1492 - accuracy: 0.9441\n",
            "Epoch 142: val_accuracy did not improve from 0.91304\n",
            "6/6 [==============================] - 1s 108ms/step - loss: 0.1492 - accuracy: 0.9441 - val_loss: 0.3555 - val_accuracy: 0.8859\n",
            "Epoch 143/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1575 - accuracy: 0.9413\n",
            "Epoch 143: val_accuracy did not improve from 0.91304\n",
            "6/6 [==============================] - 0s 80ms/step - loss: 0.1575 - accuracy: 0.9413 - val_loss: 0.3460 - val_accuracy: 0.8913\n",
            "Epoch 144/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1478 - accuracy: 0.9454\n",
            "Epoch 144: val_accuracy did not improve from 0.91304\n",
            "6/6 [==============================] - 1s 99ms/step - loss: 0.1478 - accuracy: 0.9454 - val_loss: 0.3471 - val_accuracy: 0.8967\n",
            "Epoch 145/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1554 - accuracy: 0.9359\n",
            "Epoch 145: val_accuracy did not improve from 0.91304\n",
            "6/6 [==============================] - 0s 83ms/step - loss: 0.1554 - accuracy: 0.9359 - val_loss: 0.3384 - val_accuracy: 0.9130\n",
            "Epoch 146/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1602 - accuracy: 0.9386\n",
            "Epoch 146: val_accuracy did not improve from 0.91304\n",
            "6/6 [==============================] - 0s 81ms/step - loss: 0.1602 - accuracy: 0.9386 - val_loss: 0.3566 - val_accuracy: 0.8913\n",
            "Epoch 147/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1357 - accuracy: 0.9454\n",
            "Epoch 147: val_accuracy did not improve from 0.91304\n",
            "6/6 [==============================] - 0s 81ms/step - loss: 0.1357 - accuracy: 0.9454 - val_loss: 0.3330 - val_accuracy: 0.9022\n",
            "Epoch 148/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1531 - accuracy: 0.9345\n",
            "Epoch 148: val_accuracy did not improve from 0.91304\n",
            "6/6 [==============================] - 0s 81ms/step - loss: 0.1531 - accuracy: 0.9345 - val_loss: 0.3469 - val_accuracy: 0.8913\n",
            "Epoch 149/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1462 - accuracy: 0.9372\n",
            "Epoch 149: val_accuracy did not improve from 0.91304\n",
            "6/6 [==============================] - 0s 84ms/step - loss: 0.1462 - accuracy: 0.9372 - val_loss: 0.3259 - val_accuracy: 0.9076\n",
            "Epoch 150/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1400 - accuracy: 0.9400\n",
            "Epoch 150: val_accuracy did not improve from 0.91304\n",
            "6/6 [==============================] - 0s 83ms/step - loss: 0.1400 - accuracy: 0.9400 - val_loss: 0.4029 - val_accuracy: 0.8478\n",
            "Epoch 151/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1573 - accuracy: 0.9291\n",
            "Epoch 151: val_accuracy did not improve from 0.91304\n",
            "6/6 [==============================] - 0s 82ms/step - loss: 0.1573 - accuracy: 0.9291 - val_loss: 0.3750 - val_accuracy: 0.8750\n",
            "Epoch 152/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1657 - accuracy: 0.9386\n",
            "Epoch 152: val_accuracy did not improve from 0.91304\n",
            "6/6 [==============================] - 0s 82ms/step - loss: 0.1657 - accuracy: 0.9386 - val_loss: 0.3285 - val_accuracy: 0.9076\n",
            "Epoch 153/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1380 - accuracy: 0.9509\n",
            "Epoch 153: val_accuracy did not improve from 0.91304\n",
            "6/6 [==============================] - 0s 82ms/step - loss: 0.1380 - accuracy: 0.9509 - val_loss: 0.3628 - val_accuracy: 0.8967\n",
            "Epoch 154/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1448 - accuracy: 0.9332\n",
            "Epoch 154: val_accuracy did not improve from 0.91304\n",
            "6/6 [==============================] - 0s 83ms/step - loss: 0.1448 - accuracy: 0.9332 - val_loss: 0.3756 - val_accuracy: 0.8859\n",
            "Epoch 155/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1389 - accuracy: 0.9454\n",
            "Epoch 155: val_accuracy did not improve from 0.91304\n",
            "6/6 [==============================] - 0s 82ms/step - loss: 0.1389 - accuracy: 0.9454 - val_loss: 0.3485 - val_accuracy: 0.9076\n",
            "Epoch 156/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1327 - accuracy: 0.9509\n",
            "Epoch 156: val_accuracy did not improve from 0.91304\n",
            "6/6 [==============================] - 0s 83ms/step - loss: 0.1327 - accuracy: 0.9509 - val_loss: 0.3367 - val_accuracy: 0.9130\n",
            "Epoch 157/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1336 - accuracy: 0.9523\n",
            "Epoch 157: val_accuracy did not improve from 0.91304\n",
            "6/6 [==============================] - 0s 81ms/step - loss: 0.1336 - accuracy: 0.9523 - val_loss: 0.3524 - val_accuracy: 0.8967\n",
            "Epoch 158/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1352 - accuracy: 0.9468\n",
            "Epoch 158: val_accuracy did not improve from 0.91304\n",
            "6/6 [==============================] - 0s 81ms/step - loss: 0.1352 - accuracy: 0.9468 - val_loss: 0.3606 - val_accuracy: 0.8913\n",
            "Epoch 159/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1318 - accuracy: 0.9454\n",
            "Epoch 159: val_accuracy did not improve from 0.91304\n",
            "6/6 [==============================] - 0s 82ms/step - loss: 0.1318 - accuracy: 0.9454 - val_loss: 0.3888 - val_accuracy: 0.8696\n",
            "Epoch 160/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1255 - accuracy: 0.9482\n",
            "Epoch 160: val_accuracy did not improve from 0.91304\n",
            "6/6 [==============================] - 0s 83ms/step - loss: 0.1255 - accuracy: 0.9482 - val_loss: 0.4246 - val_accuracy: 0.8424\n",
            "Epoch 161/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1236 - accuracy: 0.9536\n",
            "Epoch 161: val_accuracy did not improve from 0.91304\n",
            "6/6 [==============================] - 0s 82ms/step - loss: 0.1236 - accuracy: 0.9536 - val_loss: 0.3820 - val_accuracy: 0.8696\n",
            "Epoch 162/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1549 - accuracy: 0.9386\n",
            "Epoch 162: val_accuracy did not improve from 0.91304\n",
            "6/6 [==============================] - 0s 80ms/step - loss: 0.1549 - accuracy: 0.9386 - val_loss: 0.3470 - val_accuracy: 0.8913\n",
            "Epoch 163/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1374 - accuracy: 0.9482\n",
            "Epoch 163: val_accuracy did not improve from 0.91304\n",
            "6/6 [==============================] - 0s 83ms/step - loss: 0.1374 - accuracy: 0.9482 - val_loss: 0.3365 - val_accuracy: 0.8859\n",
            "Epoch 164/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1503 - accuracy: 0.9386\n",
            "Epoch 164: val_accuracy did not improve from 0.91304\n",
            "6/6 [==============================] - 0s 82ms/step - loss: 0.1503 - accuracy: 0.9386 - val_loss: 0.4095 - val_accuracy: 0.8478\n",
            "Epoch 165/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1601 - accuracy: 0.9345\n",
            "Epoch 165: val_accuracy did not improve from 0.91304\n",
            "6/6 [==============================] - 0s 84ms/step - loss: 0.1601 - accuracy: 0.9345 - val_loss: 0.4278 - val_accuracy: 0.8478\n",
            "Epoch 166/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1541 - accuracy: 0.9454\n",
            "Epoch 166: val_accuracy did not improve from 0.91304\n",
            "6/6 [==============================] - 0s 82ms/step - loss: 0.1541 - accuracy: 0.9454 - val_loss: 0.3334 - val_accuracy: 0.8913\n",
            "Epoch 167/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1406 - accuracy: 0.9454\n",
            "Epoch 167: val_accuracy did not improve from 0.91304\n",
            "6/6 [==============================] - 0s 82ms/step - loss: 0.1406 - accuracy: 0.9454 - val_loss: 0.4242 - val_accuracy: 0.8641\n",
            "Epoch 168/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1332 - accuracy: 0.9468\n",
            "Epoch 168: val_accuracy did not improve from 0.91304\n",
            "6/6 [==============================] - 0s 84ms/step - loss: 0.1332 - accuracy: 0.9468 - val_loss: 0.3245 - val_accuracy: 0.8967\n",
            "Epoch 169/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1214 - accuracy: 0.9577\n",
            "Epoch 169: val_accuracy did not improve from 0.91304\n",
            "6/6 [==============================] - 0s 82ms/step - loss: 0.1214 - accuracy: 0.9577 - val_loss: 0.4130 - val_accuracy: 0.8533\n",
            "Epoch 170/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1209 - accuracy: 0.9495\n",
            "Epoch 170: val_accuracy did not improve from 0.91304\n",
            "6/6 [==============================] - 0s 81ms/step - loss: 0.1209 - accuracy: 0.9495 - val_loss: 0.3363 - val_accuracy: 0.8859\n",
            "Epoch 171/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1211 - accuracy: 0.9509\n",
            "Epoch 171: val_accuracy did not improve from 0.91304\n",
            "6/6 [==============================] - 0s 81ms/step - loss: 0.1211 - accuracy: 0.9509 - val_loss: 0.4637 - val_accuracy: 0.8043\n",
            "Epoch 172/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1435 - accuracy: 0.9372\n",
            "Epoch 172: val_accuracy did not improve from 0.91304\n",
            "6/6 [==============================] - 0s 83ms/step - loss: 0.1435 - accuracy: 0.9372 - val_loss: 0.3714 - val_accuracy: 0.8750\n",
            "Epoch 173/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1341 - accuracy: 0.9441\n",
            "Epoch 173: val_accuracy did not improve from 0.91304\n",
            "6/6 [==============================] - 0s 82ms/step - loss: 0.1341 - accuracy: 0.9441 - val_loss: 0.3303 - val_accuracy: 0.8967\n",
            "Epoch 174/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1401 - accuracy: 0.9400\n",
            "Epoch 174: val_accuracy did not improve from 0.91304\n",
            "6/6 [==============================] - 0s 84ms/step - loss: 0.1401 - accuracy: 0.9400 - val_loss: 0.3652 - val_accuracy: 0.8913\n",
            "Epoch 175/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1425 - accuracy: 0.9454\n",
            "Epoch 175: val_accuracy did not improve from 0.91304\n",
            "6/6 [==============================] - 0s 83ms/step - loss: 0.1425 - accuracy: 0.9454 - val_loss: 0.3993 - val_accuracy: 0.8533\n",
            "Epoch 176/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1163 - accuracy: 0.9577\n",
            "Epoch 176: val_accuracy did not improve from 0.91304\n",
            "6/6 [==============================] - 0s 83ms/step - loss: 0.1163 - accuracy: 0.9577 - val_loss: 0.3967 - val_accuracy: 0.8641\n",
            "Epoch 177/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1136 - accuracy: 0.9509\n",
            "Epoch 177: val_accuracy did not improve from 0.91304\n",
            "6/6 [==============================] - 0s 82ms/step - loss: 0.1136 - accuracy: 0.9509 - val_loss: 0.3424 - val_accuracy: 0.9076\n",
            "Epoch 178/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1191 - accuracy: 0.9454\n",
            "Epoch 178: val_accuracy did not improve from 0.91304\n",
            "6/6 [==============================] - 0s 86ms/step - loss: 0.1191 - accuracy: 0.9454 - val_loss: 0.3720 - val_accuracy: 0.8859\n",
            "Epoch 179/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1116 - accuracy: 0.9591\n",
            "Epoch 179: val_accuracy did not improve from 0.91304\n",
            "6/6 [==============================] - 0s 85ms/step - loss: 0.1116 - accuracy: 0.9591 - val_loss: 0.3407 - val_accuracy: 0.9076\n",
            "Epoch 180/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1092 - accuracy: 0.9591\n",
            "Epoch 180: val_accuracy did not improve from 0.91304\n",
            "6/6 [==============================] - 0s 85ms/step - loss: 0.1092 - accuracy: 0.9591 - val_loss: 0.3534 - val_accuracy: 0.8913\n",
            "Epoch 181/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1096 - accuracy: 0.9550\n",
            "Epoch 181: val_accuracy did not improve from 0.91304\n",
            "6/6 [==============================] - 0s 85ms/step - loss: 0.1096 - accuracy: 0.9550 - val_loss: 0.3537 - val_accuracy: 0.8913\n",
            "Epoch 182/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1057 - accuracy: 0.9591\n",
            "Epoch 182: val_accuracy did not improve from 0.91304\n",
            "6/6 [==============================] - 0s 84ms/step - loss: 0.1057 - accuracy: 0.9591 - val_loss: 0.3670 - val_accuracy: 0.8859\n",
            "Epoch 183/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1181 - accuracy: 0.9468\n",
            "Epoch 183: val_accuracy did not improve from 0.91304\n",
            "6/6 [==============================] - 0s 86ms/step - loss: 0.1181 - accuracy: 0.9468 - val_loss: 0.3702 - val_accuracy: 0.8859\n",
            "Epoch 184/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1069 - accuracy: 0.9618\n",
            "Epoch 184: val_accuracy did not improve from 0.91304\n",
            "6/6 [==============================] - 0s 86ms/step - loss: 0.1069 - accuracy: 0.9618 - val_loss: 0.3589 - val_accuracy: 0.8967\n",
            "Epoch 185/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1184 - accuracy: 0.9618\n",
            "Epoch 185: val_accuracy did not improve from 0.91304\n",
            "6/6 [==============================] - 1s 85ms/step - loss: 0.1184 - accuracy: 0.9618 - val_loss: 0.3729 - val_accuracy: 0.8750\n",
            "Epoch 186/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1133 - accuracy: 0.9550\n",
            "Epoch 186: val_accuracy did not improve from 0.91304\n",
            "6/6 [==============================] - 0s 86ms/step - loss: 0.1133 - accuracy: 0.9550 - val_loss: 0.4435 - val_accuracy: 0.8370\n",
            "Epoch 187/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1093 - accuracy: 0.9591\n",
            "Epoch 187: val_accuracy did not improve from 0.91304\n",
            "6/6 [==============================] - 1s 88ms/step - loss: 0.1093 - accuracy: 0.9591 - val_loss: 0.3294 - val_accuracy: 0.8913\n",
            "Epoch 188/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1201 - accuracy: 0.9591\n",
            "Epoch 188: val_accuracy did not improve from 0.91304\n",
            "6/6 [==============================] - 0s 82ms/step - loss: 0.1201 - accuracy: 0.9591 - val_loss: 0.3731 - val_accuracy: 0.8967\n",
            "Epoch 189/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1286 - accuracy: 0.9509\n",
            "Epoch 189: val_accuracy did not improve from 0.91304\n",
            "6/6 [==============================] - 0s 84ms/step - loss: 0.1286 - accuracy: 0.9509 - val_loss: 0.3994 - val_accuracy: 0.8533\n",
            "Epoch 190/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1206 - accuracy: 0.9509\n",
            "Epoch 190: val_accuracy did not improve from 0.91304\n",
            "6/6 [==============================] - 1s 86ms/step - loss: 0.1206 - accuracy: 0.9509 - val_loss: 0.3592 - val_accuracy: 0.8967\n",
            "Epoch 191/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1106 - accuracy: 0.9563\n",
            "Epoch 191: val_accuracy did not improve from 0.91304\n",
            "6/6 [==============================] - 0s 83ms/step - loss: 0.1106 - accuracy: 0.9563 - val_loss: 0.4147 - val_accuracy: 0.8533\n",
            "Epoch 192/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1173 - accuracy: 0.9523\n",
            "Epoch 192: val_accuracy did not improve from 0.91304\n",
            "6/6 [==============================] - 0s 83ms/step - loss: 0.1173 - accuracy: 0.9523 - val_loss: 0.3721 - val_accuracy: 0.8696\n",
            "Epoch 193/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1154 - accuracy: 0.9495\n",
            "Epoch 193: val_accuracy did not improve from 0.91304\n",
            "6/6 [==============================] - 0s 82ms/step - loss: 0.1154 - accuracy: 0.9495 - val_loss: 0.3608 - val_accuracy: 0.8967\n",
            "Epoch 194/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0948 - accuracy: 0.9645\n",
            "Epoch 194: val_accuracy did not improve from 0.91304\n",
            "6/6 [==============================] - 1s 88ms/step - loss: 0.0948 - accuracy: 0.9645 - val_loss: 0.4373 - val_accuracy: 0.8424\n",
            "Epoch 195/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1232 - accuracy: 0.9468\n",
            "Epoch 195: val_accuracy did not improve from 0.91304\n",
            "6/6 [==============================] - 0s 85ms/step - loss: 0.1232 - accuracy: 0.9468 - val_loss: 0.4362 - val_accuracy: 0.8587\n",
            "Epoch 196/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1115 - accuracy: 0.9632\n",
            "Epoch 196: val_accuracy did not improve from 0.91304\n",
            "6/6 [==============================] - 1s 86ms/step - loss: 0.1115 - accuracy: 0.9632 - val_loss: 0.3374 - val_accuracy: 0.9076\n",
            "Epoch 197/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1197 - accuracy: 0.9509\n",
            "Epoch 197: val_accuracy did not improve from 0.91304\n",
            "6/6 [==============================] - 0s 83ms/step - loss: 0.1197 - accuracy: 0.9509 - val_loss: 0.3918 - val_accuracy: 0.8696\n",
            "Epoch 198/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1024 - accuracy: 0.9577\n",
            "Epoch 198: val_accuracy did not improve from 0.91304\n",
            "6/6 [==============================] - 0s 82ms/step - loss: 0.1024 - accuracy: 0.9577 - val_loss: 0.4968 - val_accuracy: 0.8261\n",
            "Epoch 199/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1048 - accuracy: 0.9604\n",
            "Epoch 199: val_accuracy did not improve from 0.91304\n",
            "6/6 [==============================] - 0s 86ms/step - loss: 0.1048 - accuracy: 0.9604 - val_loss: 0.3621 - val_accuracy: 0.8913\n",
            "Epoch 200/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1062 - accuracy: 0.9550\n",
            "Epoch 200: val_accuracy did not improve from 0.91304\n",
            "6/6 [==============================] - 1s 86ms/step - loss: 0.1062 - accuracy: 0.9550 - val_loss: 0.3581 - val_accuracy: 0.8913\n",
            "Epoch 201/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0868 - accuracy: 0.9714\n",
            "Epoch 201: val_accuracy did not improve from 0.91304\n",
            "6/6 [==============================] - 0s 84ms/step - loss: 0.0868 - accuracy: 0.9714 - val_loss: 0.4140 - val_accuracy: 0.8533\n",
            "Epoch 202/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0897 - accuracy: 0.9727\n",
            "Epoch 202: val_accuracy improved from 0.91304 to 0.91848, saving model to mymodel2_202.h5\n",
            "6/6 [==============================] - 1s 94ms/step - loss: 0.0897 - accuracy: 0.9727 - val_loss: 0.3327 - val_accuracy: 0.9185\n",
            "Epoch 203/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0963 - accuracy: 0.9645\n",
            "Epoch 203: val_accuracy did not improve from 0.91848\n",
            "6/6 [==============================] - 0s 84ms/step - loss: 0.0963 - accuracy: 0.9645 - val_loss: 0.4494 - val_accuracy: 0.8370\n",
            "Epoch 204/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0976 - accuracy: 0.9659\n",
            "Epoch 204: val_accuracy did not improve from 0.91848\n",
            "6/6 [==============================] - 0s 85ms/step - loss: 0.0976 - accuracy: 0.9659 - val_loss: 0.3764 - val_accuracy: 0.8859\n",
            "Epoch 205/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0951 - accuracy: 0.9632\n",
            "Epoch 205: val_accuracy did not improve from 0.91848\n",
            "6/6 [==============================] - 0s 86ms/step - loss: 0.0951 - accuracy: 0.9632 - val_loss: 0.3690 - val_accuracy: 0.8967\n",
            "Epoch 206/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1130 - accuracy: 0.9563\n",
            "Epoch 206: val_accuracy did not improve from 0.91848\n",
            "6/6 [==============================] - 0s 85ms/step - loss: 0.1130 - accuracy: 0.9563 - val_loss: 0.4477 - val_accuracy: 0.8478\n",
            "Epoch 207/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1044 - accuracy: 0.9563\n",
            "Epoch 207: val_accuracy did not improve from 0.91848\n",
            "6/6 [==============================] - 1s 90ms/step - loss: 0.1044 - accuracy: 0.9563 - val_loss: 0.4751 - val_accuracy: 0.8261\n",
            "Epoch 208/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1027 - accuracy: 0.9673\n",
            "Epoch 208: val_accuracy did not improve from 0.91848\n",
            "6/6 [==============================] - 0s 84ms/step - loss: 0.1027 - accuracy: 0.9673 - val_loss: 0.3636 - val_accuracy: 0.9130\n",
            "Epoch 209/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0972 - accuracy: 0.9563\n",
            "Epoch 209: val_accuracy did not improve from 0.91848\n",
            "6/6 [==============================] - 0s 84ms/step - loss: 0.0972 - accuracy: 0.9563 - val_loss: 0.3548 - val_accuracy: 0.8859\n",
            "Epoch 210/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0927 - accuracy: 0.9645\n",
            "Epoch 210: val_accuracy did not improve from 0.91848\n",
            "6/6 [==============================] - 0s 84ms/step - loss: 0.0927 - accuracy: 0.9645 - val_loss: 0.4286 - val_accuracy: 0.8750\n",
            "Epoch 211/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0964 - accuracy: 0.9591\n",
            "Epoch 211: val_accuracy did not improve from 0.91848\n",
            "6/6 [==============================] - 0s 83ms/step - loss: 0.0964 - accuracy: 0.9591 - val_loss: 0.4306 - val_accuracy: 0.8370\n",
            "Epoch 212/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1080 - accuracy: 0.9632\n",
            "Epoch 212: val_accuracy did not improve from 0.91848\n",
            "6/6 [==============================] - 0s 85ms/step - loss: 0.1080 - accuracy: 0.9632 - val_loss: 0.3344 - val_accuracy: 0.8967\n",
            "Epoch 213/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0943 - accuracy: 0.9686\n",
            "Epoch 213: val_accuracy did not improve from 0.91848\n",
            "6/6 [==============================] - 0s 85ms/step - loss: 0.0943 - accuracy: 0.9686 - val_loss: 0.3449 - val_accuracy: 0.8913\n",
            "Epoch 214/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0877 - accuracy: 0.9659\n",
            "Epoch 214: val_accuracy did not improve from 0.91848\n",
            "6/6 [==============================] - 0s 83ms/step - loss: 0.0877 - accuracy: 0.9659 - val_loss: 0.4280 - val_accuracy: 0.8424\n",
            "Epoch 215/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0856 - accuracy: 0.9591\n",
            "Epoch 215: val_accuracy did not improve from 0.91848\n",
            "6/6 [==============================] - 0s 85ms/step - loss: 0.0856 - accuracy: 0.9591 - val_loss: 0.3536 - val_accuracy: 0.8913\n",
            "Epoch 216/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0957 - accuracy: 0.9659\n",
            "Epoch 216: val_accuracy did not improve from 0.91848\n",
            "6/6 [==============================] - 0s 82ms/step - loss: 0.0957 - accuracy: 0.9659 - val_loss: 0.3141 - val_accuracy: 0.9185\n",
            "Epoch 217/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0913 - accuracy: 0.9618\n",
            "Epoch 217: val_accuracy did not improve from 0.91848\n",
            "6/6 [==============================] - 0s 84ms/step - loss: 0.0913 - accuracy: 0.9618 - val_loss: 0.4462 - val_accuracy: 0.8641\n",
            "Epoch 218/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0916 - accuracy: 0.9632\n",
            "Epoch 218: val_accuracy did not improve from 0.91848\n",
            "6/6 [==============================] - 0s 84ms/step - loss: 0.0916 - accuracy: 0.9632 - val_loss: 0.3924 - val_accuracy: 0.8641\n",
            "Epoch 219/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0804 - accuracy: 0.9714\n",
            "Epoch 219: val_accuracy did not improve from 0.91848\n",
            "6/6 [==============================] - 0s 84ms/step - loss: 0.0804 - accuracy: 0.9714 - val_loss: 0.4542 - val_accuracy: 0.8424\n",
            "Epoch 220/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0846 - accuracy: 0.9673\n",
            "Epoch 220: val_accuracy did not improve from 0.91848\n",
            "6/6 [==============================] - 0s 86ms/step - loss: 0.0846 - accuracy: 0.9673 - val_loss: 0.3259 - val_accuracy: 0.9076\n",
            "Epoch 221/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0871 - accuracy: 0.9604\n",
            "Epoch 221: val_accuracy did not improve from 0.91848\n",
            "6/6 [==============================] - 0s 84ms/step - loss: 0.0871 - accuracy: 0.9604 - val_loss: 0.4833 - val_accuracy: 0.8478\n",
            "Epoch 222/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0739 - accuracy: 0.9754\n",
            "Epoch 222: val_accuracy did not improve from 0.91848\n",
            "6/6 [==============================] - 0s 83ms/step - loss: 0.0739 - accuracy: 0.9754 - val_loss: 0.3268 - val_accuracy: 0.8859\n",
            "Epoch 223/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0821 - accuracy: 0.9700\n",
            "Epoch 223: val_accuracy did not improve from 0.91848\n",
            "6/6 [==============================] - 0s 85ms/step - loss: 0.0821 - accuracy: 0.9700 - val_loss: 0.4468 - val_accuracy: 0.8424\n",
            "Epoch 224/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0950 - accuracy: 0.9673\n",
            "Epoch 224: val_accuracy did not improve from 0.91848\n",
            "6/6 [==============================] - 1s 91ms/step - loss: 0.0950 - accuracy: 0.9673 - val_loss: 0.3695 - val_accuracy: 0.8913\n",
            "Epoch 225/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0819 - accuracy: 0.9686\n",
            "Epoch 225: val_accuracy did not improve from 0.91848\n",
            "6/6 [==============================] - 1s 87ms/step - loss: 0.0819 - accuracy: 0.9686 - val_loss: 0.3778 - val_accuracy: 0.8641\n",
            "Epoch 226/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0798 - accuracy: 0.9754\n",
            "Epoch 226: val_accuracy did not improve from 0.91848\n",
            "6/6 [==============================] - 0s 85ms/step - loss: 0.0798 - accuracy: 0.9754 - val_loss: 0.3898 - val_accuracy: 0.8967\n",
            "Epoch 227/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0723 - accuracy: 0.9809\n",
            "Epoch 227: val_accuracy did not improve from 0.91848\n",
            "6/6 [==============================] - 0s 84ms/step - loss: 0.0723 - accuracy: 0.9809 - val_loss: 0.4216 - val_accuracy: 0.8641\n",
            "Epoch 228/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0688 - accuracy: 0.9768\n",
            "Epoch 228: val_accuracy did not improve from 0.91848\n",
            "6/6 [==============================] - 0s 84ms/step - loss: 0.0688 - accuracy: 0.9768 - val_loss: 0.3407 - val_accuracy: 0.9130\n",
            "Epoch 229/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0735 - accuracy: 0.9782\n",
            "Epoch 229: val_accuracy did not improve from 0.91848\n",
            "6/6 [==============================] - 1s 88ms/step - loss: 0.0735 - accuracy: 0.9782 - val_loss: 0.4249 - val_accuracy: 0.8641\n",
            "Epoch 230/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0700 - accuracy: 0.9741\n",
            "Epoch 230: val_accuracy did not improve from 0.91848\n",
            "6/6 [==============================] - 0s 84ms/step - loss: 0.0700 - accuracy: 0.9741 - val_loss: 0.4287 - val_accuracy: 0.8533\n",
            "Epoch 231/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0661 - accuracy: 0.9754\n",
            "Epoch 231: val_accuracy did not improve from 0.91848\n",
            "6/6 [==============================] - 0s 85ms/step - loss: 0.0661 - accuracy: 0.9754 - val_loss: 0.4208 - val_accuracy: 0.8587\n",
            "Epoch 232/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0664 - accuracy: 0.9795\n",
            "Epoch 232: val_accuracy did not improve from 0.91848\n",
            "6/6 [==============================] - 0s 83ms/step - loss: 0.0664 - accuracy: 0.9795 - val_loss: 0.4322 - val_accuracy: 0.8587\n",
            "Epoch 233/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0709 - accuracy: 0.9727\n",
            "Epoch 233: val_accuracy did not improve from 0.91848\n",
            "6/6 [==============================] - 1s 86ms/step - loss: 0.0709 - accuracy: 0.9727 - val_loss: 0.3788 - val_accuracy: 0.9076\n",
            "Epoch 234/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0787 - accuracy: 0.9659\n",
            "Epoch 234: val_accuracy did not improve from 0.91848\n",
            "6/6 [==============================] - 0s 85ms/step - loss: 0.0787 - accuracy: 0.9659 - val_loss: 0.4552 - val_accuracy: 0.8696\n",
            "Epoch 235/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0744 - accuracy: 0.9714\n",
            "Epoch 235: val_accuracy did not improve from 0.91848\n",
            "6/6 [==============================] - 1s 86ms/step - loss: 0.0744 - accuracy: 0.9714 - val_loss: 0.4830 - val_accuracy: 0.8261\n",
            "Epoch 236/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0762 - accuracy: 0.9768\n",
            "Epoch 236: val_accuracy did not improve from 0.91848\n",
            "6/6 [==============================] - 0s 83ms/step - loss: 0.0762 - accuracy: 0.9768 - val_loss: 0.4394 - val_accuracy: 0.8750\n",
            "Epoch 237/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0649 - accuracy: 0.9809\n",
            "Epoch 237: val_accuracy did not improve from 0.91848\n",
            "6/6 [==============================] - 1s 86ms/step - loss: 0.0649 - accuracy: 0.9809 - val_loss: 0.3390 - val_accuracy: 0.8967\n",
            "Epoch 238/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0812 - accuracy: 0.9714\n",
            "Epoch 238: val_accuracy did not improve from 0.91848\n",
            "6/6 [==============================] - 1s 86ms/step - loss: 0.0812 - accuracy: 0.9714 - val_loss: 0.4676 - val_accuracy: 0.8587\n",
            "Epoch 239/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0880 - accuracy: 0.9604\n",
            "Epoch 239: val_accuracy did not improve from 0.91848\n",
            "6/6 [==============================] - 1s 88ms/step - loss: 0.0880 - accuracy: 0.9604 - val_loss: 0.4066 - val_accuracy: 0.8696\n",
            "Epoch 240/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0694 - accuracy: 0.9782\n",
            "Epoch 240: val_accuracy did not improve from 0.91848\n",
            "6/6 [==============================] - 0s 86ms/step - loss: 0.0694 - accuracy: 0.9782 - val_loss: 0.3776 - val_accuracy: 0.9022\n",
            "Epoch 241/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0559 - accuracy: 0.9836\n",
            "Epoch 241: val_accuracy did not improve from 0.91848\n",
            "6/6 [==============================] - 1s 86ms/step - loss: 0.0559 - accuracy: 0.9836 - val_loss: 0.4203 - val_accuracy: 0.8804\n",
            "Epoch 242/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0685 - accuracy: 0.9741\n",
            "Epoch 242: val_accuracy did not improve from 0.91848\n",
            "6/6 [==============================] - 1s 87ms/step - loss: 0.0685 - accuracy: 0.9741 - val_loss: 0.4398 - val_accuracy: 0.8750\n",
            "Epoch 243/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0634 - accuracy: 0.9809\n",
            "Epoch 243: val_accuracy did not improve from 0.91848\n",
            "6/6 [==============================] - 0s 86ms/step - loss: 0.0634 - accuracy: 0.9809 - val_loss: 0.4231 - val_accuracy: 0.8750\n",
            "Epoch 244/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0705 - accuracy: 0.9768\n",
            "Epoch 244: val_accuracy did not improve from 0.91848\n",
            "6/6 [==============================] - 1s 87ms/step - loss: 0.0705 - accuracy: 0.9768 - val_loss: 0.3699 - val_accuracy: 0.9076\n",
            "Epoch 245/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0649 - accuracy: 0.9754\n",
            "Epoch 245: val_accuracy did not improve from 0.91848\n",
            "6/6 [==============================] - 1s 94ms/step - loss: 0.0649 - accuracy: 0.9754 - val_loss: 0.4541 - val_accuracy: 0.8696\n",
            "Epoch 246/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0631 - accuracy: 0.9768\n",
            "Epoch 246: val_accuracy did not improve from 0.91848\n",
            "6/6 [==============================] - 1s 91ms/step - loss: 0.0631 - accuracy: 0.9768 - val_loss: 0.5200 - val_accuracy: 0.8261\n",
            "Epoch 247/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0746 - accuracy: 0.9741\n",
            "Epoch 247: val_accuracy did not improve from 0.91848\n",
            "6/6 [==============================] - 1s 94ms/step - loss: 0.0746 - accuracy: 0.9741 - val_loss: 0.4603 - val_accuracy: 0.8533\n",
            "Epoch 248/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0582 - accuracy: 0.9782\n",
            "Epoch 248: val_accuracy did not improve from 0.91848\n",
            "6/6 [==============================] - 1s 90ms/step - loss: 0.0582 - accuracy: 0.9782 - val_loss: 0.3887 - val_accuracy: 0.9022\n",
            "Epoch 249/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0729 - accuracy: 0.9727\n",
            "Epoch 249: val_accuracy did not improve from 0.91848\n",
            "6/6 [==============================] - 1s 95ms/step - loss: 0.0729 - accuracy: 0.9727 - val_loss: 0.3922 - val_accuracy: 0.8913\n",
            "Epoch 250/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0611 - accuracy: 0.9809\n",
            "Epoch 250: val_accuracy did not improve from 0.91848\n",
            "6/6 [==============================] - 1s 87ms/step - loss: 0.0611 - accuracy: 0.9809 - val_loss: 0.3687 - val_accuracy: 0.9076\n",
            "Training completed in time:  0:02:22.051602\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluating the model on the training and testing set\n",
        "score = model.evaluate(x_train, y_train, verbose=0)\n",
        "print(\"Training Accuracy: \", score[1])\n",
        "\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "print(\"Testing Accuracy: \", score[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ZG83L3ZyQPE",
        "outputId": "98368eed-4ea4-406b-de71-f65380891fc0"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Accuracy:  0.9754433631896973\n",
            "Testing Accuracy:  0.907608687877655\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "preds = model.predict(x_test) # label scores \n",
        "\n",
        "classpreds = np.argmax(preds, axis=1) # predicted classes \n",
        "\n",
        "y_testclass = np.argmax(y_test, axis=1) # true classes\n",
        "\n",
        "n_classes=6 # number of classes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NFR5sf22yr0U",
        "outputId": "5c25f760-dd51-425f-8424-87e560c058d5"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6/6 [==============================] - 0s 8ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute ROC curve and ROC area for each class\n",
        "fpr = dict()\n",
        "tpr = dict()\n",
        "roc_auc = dict()\n",
        "for i in range(n_classes):\n",
        "    fpr[i], tpr[i], _ = roc_curve(y_test[:, i], preds[:, i])\n",
        "    roc_auc[i] = auc(fpr[i], tpr[i])"
      ],
      "metadata": {
        "id": "_StdNR35yteX"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "c_names = ['Bronchiectasis', 'Bronchiolitis', 'COPD', 'Healthy', 'Pneumonia', 'URTI']"
      ],
      "metadata": {
        "id": "SmdIR6rnyvac"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Classification Report\n",
        "print(classification_report(y_testclass, classpreds, target_names=c_names))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ly3QfOlay2dN",
        "outputId": "4066e5ba-842a-4f2f-a24b-a82464d83915"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                precision    recall  f1-score   support\n",
            "\n",
            "Bronchiectasis       1.00      0.33      0.50         3\n",
            " Bronchiolitis       0.25      0.33      0.29         3\n",
            "          COPD       0.97      0.96      0.97       159\n",
            "       Healthy       1.00      0.57      0.73         7\n",
            "     Pneumonia       0.47      1.00      0.64         7\n",
            "          URTI       0.50      0.20      0.29         5\n",
            "\n",
            "      accuracy                           0.91       184\n",
            "     macro avg       0.70      0.57      0.57       184\n",
            "  weighted avg       0.93      0.91      0.91       184\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "X6u25lbiy28s"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
